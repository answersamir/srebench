You are an expert system designed to generate synthetic incident scenarios for an AIOps benchmark called SREBench using available tools. Your task is to create a realistic-looking, albeit simplified for this Proof of Concept (POC), incident scenario for the Google Cloud Microservice Demo Kubernetes cluster.

The specific incident type is **Resource Saturation (CPU Limit)**.
The **Root Cause** is a low CPU limit configured on the `paymentservice` deployment. Let's assume the limit is set to `50m`.
This low limit causes throttling and performance degradation under load, impacting services that depend on `paymentservice`.

**General Constraints for State Data (CRITICAL):**

* The `state/` data files (`logs.jsonl`, `metrics.json`, `events.jsonl`, `topology.json`, `configuration.yaml`) **MUST ONLY** contain representations of observable symptoms and low-level operational data.
* **DO NOT** embed explicit statements about the root cause or diagnostic conclusions (e.g., "this issue is caused by a low CPU limit", "Insufficient CPU resources are the root cause") within the `state/` data content.
* The agent's task is to infer the root cause and causal chain solely from analyzing the patterns and anomalies within the `state/` data, using the `description.md` as the initial symptom alert.
* The content of `state/` files **MUST strictly mimic the structured output format** of real monitoring tools and APIs (like journald/Loki for logs, Prometheus/Stackdriver time-series APIs for metrics, kubectl json/yaml output for events/topology/config). Avoid natural language summaries *within* these data files themselves. `description.md` is the exception, serving as the initial natural language alert summary.
* Ensure no comments are added in any files that might indicate about the actual RCA.

Generate the necessary files for a single scenario in the SREBench format, adhering to the constraints above and the specific file formats requested below. For this POC, focus on generating representative content rather than perfectly correlated time-series data or exhaustive logs or events.

**Files to Generate:**

1. **`scenario_<id>/description.md`**: A natural language description of the symptoms an SRE or monitoring system would initially observe. Focus on user experience issues, service-level errors/latency, and potential initial alerts.
2. **`scenario_<id>/state/logs.jsonl`**: A few sample log entries (5-10 total) across the involved services (`paymentservice`, `checkoutservice`, `cartservice`). Include typical timestamps (RFC3339), service/pod names, levels (info, warning, error), and messages that reflect the problem (e.g., messages indicating slow processing, timeouts, errors calling paymentservice). **Ensure log messages themselves do not state the root cause.** Format as newline-delimited JSON, mimicking structured log output.
3. **`scenario_<id>/state/metrics.json`**: **Generate the content for this file as a JSON object representing key metric time-series data points observed during the incident window.** Mimic the structure of time-series data retrieved from monitoring APIs. Follow this exact structure:

    ```json
    {
      "metrics": [
        {
          "name": "string", // e.g., "kubernetes_io/container/cpu/utilization_rate", "opencensus_io/http/server/latency"
          "unit": "string", // e.g., "cores", "ms", "By", "count", "ns"
          "resource": { // Resource the metric is associated with
            "name": "string", // e.g., "paymentservice"
            "kind": "string", // e.g., "Deployment", "Pod", "Service", "Node"
            "namespace": "string", // e.g., "microservices-demo", "kube-system"
            "pod_name": "string" // Optional: if metric is at pod level, ensure consistency with logs/events
          },
          "labels": {}, // Optional: Key-value pairs for metric labels (e.g., {"method": "grpc.health.v1.Health/Check"})
          "data_points": [ // Representing key query results over time
            {
              "timestamp": "RFC3339 timestamp string", // Point in time
              "value": "numeric_or_string_value" // The observed value (e.g., 0.05, 0.8, "95th_percentile=0.5s")
            }
            // Include a few points showing baseline, increase, and peak/anomalous state, consistent with logs/events timing
          ]
        }
        // ... include relevant metrics (CPU utilization, request latency, error rate) for paymentservice and dependent services.
        // Use plausible real-world metric names if known for GKE/Prometheus/opencensus.
      ]
    }
    ```

    For this scenario, include `cpu_utilization`, `request_latency` for `paymentservice`, and `request_latency`, `error_rate` for `checkoutservice` and `cartservice` towards `paymentservice`. Ensure consistency in component names and namespaces (e.g., `paymentservice` Deployment in `microservices-demo` namespace). **Remove the "summary" field.** Output only the JSON object content.
4. **`scenario_<id>/state/events.jsonl`**: Relevant Kubernetes events. Generate as newline-delimited JSON of event objects, mimicking `kubectl get events -o json`. Include plausible timestamps (RFC3339) and event details. **Ensure event messages themselves do not state the root cause conclusion.** For this CPU throttling scenario *without crashes*, this file might be empty or contain only very general events.
5. **`scenario_<id>/state/topology.json`**: A JSON array listing the relevant services involved in this incident's propagation path and their immediate dependencies, following the simplified schema discussed (`[{"service": "frontend", "calls": [...]}, ...]`). Mimics structured output describing service relationships. Use accurate service names from the Microservice Demo.
6. **`scenario_<id>/state/configuration.yaml`**: A YAML snippet showing the relevant problematic configuration â€“ specifically, the `resources.limits.cpu` setting for the `paymentservice` container within its Deployment manifest. Mimics output of `kubectl get deployment paymentservice -o yaml`. Ensure the namespace and deployment name are correct.
7. **`scenario_<id>/ground_truth/root_cause.json`**: Generate the content for this file as a JSON object describing the root cause. Follow the schema defined previously (type, resource, affected_component, etc.).
8. **`scenario_<id>/ground_truth/causal_graph.json`**: Generate the content for this file as a JSON object representing the directed causal graph of the incident. Follow the structured nodes/edges schema defined previously.
9. **`scenario_<id>/ground_truth/resolution.json`**: Generate the content for this file as a JSON object describing the primary resolution. Follow the schema defined previously (action_type, target_component, etc.).
10. **`scenario_<id>/metadata.json`**: A JSON object containing basic metadata like `incident_type: "Resource Saturation"`, `root_cause_type: "CPU Limit"`, `affected_services: ["paymentservice", "checkoutservice", "cartservice", "frontend"]`, `scenario_id: "scenario_<id>"`, `cluster_env: "Microservice Demo"`, `timestamp_generated: "RFC3339 timestamp of generation"`.

Ensure consistency between the root cause, causal graph, resolution, observed symptoms in the description, and the generated state data. Use plausible service and pod names from the Microservice Demo and ensure consistency in timestamps and component naming across all files (`state/` and `ground_truth/`).
